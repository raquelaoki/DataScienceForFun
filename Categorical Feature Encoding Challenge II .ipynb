{"cells":[{"metadata":{},"cell_type":"markdown","source":"# [Categorical Feature Encoding Challenge II  ](http://https://www.kaggle.com/c/cat-in-the-dat-ii)\n\nAuthor: Raquel Aoki\n\nThis is a binary classification problem that uses a large dataset made of categorical features. \nBased on the problem \"Cat in the Dat\". \n\nMy pipeline is: \n1. Join training and testing set to transform the features from categorical to dummies;\n2. Split datatsets again;\n3. Test several models (SVM, Random Forest, NN, Logist Regression)\n4. Compare their results "},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nimport pandas as pd\nimport numpy as np\nfrom sklearn import metrics, preprocessing\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import svm\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegressionCV\nimport tensorflow as tf\nfrom tensorflow import keras\n    \nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"'''Loading files'''\n#A subsample was used to speed up the tests\ntesting = True\nif testing: \n    train = pd.read_csv(\"../input/cat-in-the-dat-ii/train.csv\").sample(n=300000, random_state=1)\n    test = pd.read_csv(\"../input/cat-in-the-dat-ii/test.csv\")#.sample(n=10000, random_state=1)\n    sample = pd.read_csv(\"../input/cat-in-the-dat-ii/sample_submission.csv\")\nelse: \n    train = pd.read_csv(\"../input/cat-in-the-dat-ii/train.csv\")\n    test = pd.read_csv(\"../input/cat-in-the-dat-ii/test.csv\")\n    sample = pd.read_csv(\"../input/cat-in-the-dat-ii/sample_submission.csv\")\ndf_train = train \ndf_test = pd.DataFrame(test)\nprint(train.shape, test.shape)\n\n#Aggregating the features to transform the categorical variables \ndf_test[\"target\"] = -1\ndata = pd.concat([df_train, df_test]).reset_index(drop=True)\nprint(data.shape ,df_train.shape, df_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#transforming the categorical variables in dummies \ndid = data['id'].values\ndtarget = data['target'].values\ndata.drop(['id','target'],axis=1,inplace=True)\ncolumns = [i for i in data.columns]\ndata_new = pd.get_dummies(data,columns=columns,drop_first=True, sparse=True) \ndata_new.fillna(0)\ndel data\n\n#adding back the id and target variables\ndata_new['id'] = did\ndata_new['target'] = dtarget\nprint(data_new.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''Splitting the train and testing set after the data transformation'''\ny = np.array(data_new[data_new.target != -1].reset_index(drop=True).target)\nX = data_new[data_new.target != -1].reset_index(drop=True).drop(['target','id'], axis  = 1).to_numpy()\nX_ = data_new[data_new.target == -1].reset_index(drop=True).drop(['target','id'], axis  = 1).to_numpy()\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nModel 1: score 0.68 SVM + using a balanced dataset  \nModel 2: score 0.66 RF + using the full dataset \nModel 3: score ?? NN using full dataset \nModel 4: score 0.69 \n'''\nmodel = 'm5'\n\nif model=='m1': \n    clf = svm.SVC(kernel='rbf',gamma=0.1,C=0.3)\n    clf.fit(X_train, y_train)\n    y_val = clf.predict(X_test)\n    y_train_val = clf.predict(X_train)\n    test_preds =  clf.predict(X_)\nelif model == 'm2':    \n    rf = RandomForestClassifier(n_estimators=500, max_depth=30, class_weight={1: 5}, random_state = 42)\n    rf.fit(X_train, y_train)\n    y_val = rf.predict(X_test)\n    y_train_val = rf.predict(X_train)\n    test_preds =  rf.predict(X_)\nelif model == 'm3':\n    model = keras.Sequential([\n        keras.layers.Flatten(input_shape=(X_train.shape[1],)),\n        keras.layers.Dense(16, activation=tf.nn.relu),\n        keras.layers.Dense(16, activation=tf.nn.relu),\n        keras.layers.Dense(1, activation=tf.nn.sigmoid),\n    ])\n\n    model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n\n    model.fit(X_train, y_train, epochs=50, batch_size=1)\n    y_val = model.predict(X_test)\n    y_train_val = model.predict(X_train)\n    aux =  model.predict(X_)\n    test_preds = []\n    for i in aux: \n        test_preds.append(i[0])\nelse: \n    lr_cv = LogisticRegressionCV(Cs=7,solver=\"lbfgs\",tol=0.0001,max_iter=3000,cv=3)\n    lr_cv.fit(X_train, y_train)\n    y_val = lr_cv.predict_proba(X_test)[:, 1]\n    y_train_val = lr_cv.predict_proba(X_train)[:, 1]#lr_cv.predict(X_train)\n    test_preds =  lr_cv.predict_proba(X_)[:, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''Evaluation'''\nprint(confusion_matrix(y_train,y_train_val.astype(int)))\nprint(confusion_matrix(y_test,y_val.astype(int)))\nprint(\"Overall AUC={}\".format(metrics.roc_auc_score(y_test, y_val.astype(int))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Submission\ntest_ids = test.id.values\nprint(\"Saving submission file\")\nsubmission = pd.DataFrame({'id': test_ids,'target': test_preds})\nsubmission.to_csv(\"submission.csv\", index=False)\nsubmission.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}